## CNN's ERROR

神经网络算法之前的程序员都会很明确的知道**程序是不会错的**。如果一段代码的输出和你预想的不一样那么你总能找到那个错误的地方进行debug。而我总是很受不了这种出错的感觉，喜欢花很多时间去刨根问底的找到到底是哪里出问题导致它不对，而且我确定的知道一定能找到那个点。不过看了机器学习之后特别是神经网络算法之后我很担心以后恐怕必须接收这种黑盒问题了。

深度神经网络很大的一个缺点就是它有些地方对于我们是个黑盒。一个训练再好的模型也会把某些人类看起来完全不是猫的图片分类为猫，而这在某些应用领域是非常非常危险的，比如无人汽车。更让人烦恼的是它为什么把这个图片识别为猫还很难去解释也就是无法debug。我就在想究竟是为什么一个在大多数情况下运行很好的模型会犯这种三岁小孩都不会犯的幼稚错误。我想的解答也就是从神经网络的本质出发从概率出发。

比如一个分类问题，神经网络的输出从来就不是一个确定的值，而只是一个可能的概率。我分类它是猫只是告诉你它有99%的概率是猫，我已经说的很清楚了它只是很有**可能**是猫，但还有可能是个其他什么东西，但是如果人类硬是要忽略这个明确的概率只转述给他人是猫，那显然就不全是模型的责任了。另外一点无论多大的分类模型总不可能把所有的事物都列举清楚，你让我输出猫狗兔子三种却突然有一天给我一张杂乱的线条，我输出猫55%，狗20%，兔子25%，你说我不对太笨了，你说这又怪谁呢？最后针对CNN这种算法对于图像的识别是分块分层的，既然每一个小块都只是输出一个概率那么最终的结构就是这些概率的综合，那也许早期很小的一个扰动就会引发后面巨大错误呢。说这么多只是觉得CNN之所以会有一些离奇的错误其实就是算法本身的架构决定了的，只要算法整体架构不变是无法杜绝这个问题的。那再想想为什么我们人类不会犯这种错误呢，我们为什么绝对不会把一架飞机认成是猫呢?或许是我们有一个整体的概念，而CNN很大一部分是从细节看起。自从DP流行起来之后以前传统的拟人类思考的AI算法已经很少有人研究了，所谓分久必合或许解决DP黑箱bug问题的出路就是重新将新旧两种办法结合起来也说不定呢。

唉，人脑毕竟进化了几百万年，机器想赶上还是有不小的距离啊。