# 谷歌翻译背后的神经网络

去年九月谷歌宣布了一个轰动翻译界的重大消息，神经网络被整合进[谷歌翻译](https://translate.google.com/)并实现了翻译质量的突破性进展。到了今天因为访问谷歌翻译已不用翻墙我想许多人都已经体验到了这一技术带来的实质性效果，真的是非常的惊艳！那么这整合进去的神经网络究竟什么样子的呢，我们今天就来说道说道。

循环神经网络（[RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network)）之前有简单介绍过，本次翻译界的突破就是基于它的进一步拓展应用，论文上称之为[seq2seq](https://google.github.io/seq2seq/)。它用到的不仅仅是一个RNN网络而是两个，分别是encode和decode。seq2seq顾名思义就是sequence to sequence，也就是把一种语言的一段话先编码成一个特殊的信息，然后再用另一个RNN网络去解码这段信息最后生成另一个种语言的序列，这也就最终实现了翻译的过程。它的这种实现方式是非常符合人类大脑的翻译过程的。比如说我看到一段英文“**Hello World！**”,我的大脑接收到这段信息后理论上就会在大脑的英文处理区神经元上产生某一种特殊的电讯号，这样我就理解了这句话的意思。然后下一步我需要将它用中文翻译过来，那么这一段电讯号就会被传到我的中文处理区域，最后在我脑海中生成对应的中文语句“**你好，世界！**”。可以看到，中间的这一段编码后生成的特殊的电讯号起到了一个非常重要的承上启下的作用，它需要包含之前语言序列的所有信息。

![seq2seq-4666747073](../../mooc/deeplearning/project/deeplearing/img/seq2seq-4666747073.png)

在搭建整个模型之前我们需要找到的就是大量的训练数据，也就是一一对应的正确的中英文翻译句子，比如说电影双语字幕。如果我们要训练从英文翻译到中文，首先需要把英文的一个句子通过[词嵌入](https://en.wikipedia.org/wiki/Word_embedding)技术变成一个固定维度的向量，然后把这个向量作为input输入到encode的RNN网络中去。因为RNN的网络具有长短期记忆的能力，所以它就会自动的抽取一段话中的各种信息并不断向前传递，最后在通过最后一个神经元的$hidden$层和$content$层输出出来一个编码好的向量，这个向量就是包含了input所有信息的“密码”了。下一步我们就需要用另一个RNN网络去解码这份信息并生成翻译的结果。decode的网络接收编码得到的向量作为一种输入，另外在训练的时候为了保证翻译的准确度还需要将正确翻译的语言序列错位输入进去。这里就和一个普通的RNN网络一样了，比如输入是`<go> 你 好 ！`，那么希望的输出就是`你 好 ！<end>`。这里为什么要多一个输入进去而不是仅仅利用encode后的编码呢？接触过时间序列预测模型的人可能更有感触时间序列是相互影响的，如果你前一个预测错了那么后面基本上也就更着全错了而且会越错越离谱。对于翻译更是如此，如果我们直接使用上一个翻译的结果来作为下一个翻译的输入那么一旦上一个翻译的结果不对后面的翻译肯定也会离的越来越远。所以在训练的时候使用类似监督学习的想法，带着decode网络一点点学显然是一个更准确且训练成本低的办法。decode网络中每一个RNN的神经元输出也是一个固定维度的向量，这个向量再通过词嵌入技术可以转化为要翻译目标语言词典个数数量维度的一个向量，然后这个向量中数值最大的那个值对应的词典中的词就是我们希望得到的正确翻译的词。如此一来整个网络就构建完了，也有了优化的方向和目标，最后只要用大量的数据就可以训练出网络中的各个参数。此外需要特别注意的一点是decode层在训练后的实际使用时是不接收正确翻译词语作为input的（而且也不可能有嘛，否则还要翻译干嘛），它的input变成了encode的最终编码和自身的每一个神经元输出作为下一个神经元的输入。也就是用“你”来预测“好”，用“好”来预测“！”。整个模型看下来其实最好的就是翻译可以完全不用语法，只要有对照样本就可以实现对任何语言的翻译。难怪以前听说谷歌内部流传的说法是组内每走一名语言专家，谷歌翻译的准确度就提高10%呢（笑）。

seq2seq这种模型给人一种启发就是如何利用中间编码把一种信息转变成另一种信息。只要使用这种模型我们就可以让机器自动的把金庸风格的小说变成古龙的，或者把一篇散文自动的生成一首对应的诗词。可以举一个具体的例子，假如我要实现把一个小公司的对账单流水转变成如果它有500亿规模以后的对账单流水我可以怎么做呢。首先我要找到和原公司（简称A）同一领域的上市大公司（简称B）的现金流水。然后我可以以周为单位对A和B的流水进行配对，类似于`*[200,-300,700,60,80,-500,1000] ==> [2w,4w,-10w,9w,-25w,13w,77w,50w,-20w]`。然后我把A公司的每个sequence通过encode层进行编码，获取A公司的现金流水特征，再将该编码输入到decode层辅以B公司的现金流sequence进行训练。训练完成之后我再输入A公司新的一周的现金流水信息，就可以预测得到如果A公司有B公司那么大规模时的现金流水的样子了。最关键的是这样得到的数据是非常合理的，因为在模型的训练时我就已经利用了这两个公司的真实信息了，所以新的模型输出的流水就应该是既有A公司自身的特征又能够合理将其放大到B那个规模时可能有的流水信息了。是不是很神奇啊？Let's do it!